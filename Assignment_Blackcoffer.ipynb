{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37885b6a",
   "metadata": {},
   "source": [
    "# Explanation of Approach:\n",
    "\n",
    "The provided Python script performs sentiment analysis and calculates various text metrics on a dataset of web articles. Here's a breakdown of the approach:\n",
    "\n",
    "## 1.Data Loading:\n",
    "Reads input data from an Excel file containing URLs of web articles.\n",
    "\n",
    "## 2.Loading Dictionaries and Stopwords:\n",
    "Loads positive and negative word dictionaries.\n",
    "Loads a collection of stopwords from text files.\n",
    "\n",
    "## 3.Text Processing and Analysis:\n",
    "Utilizes the BeautifulSoup library to scrape HTML content of each article.\n",
    "Extracts article title and text content.\n",
    "Cleans and tokenizes the text, removing punctuation and stopwords.\n",
    "Calculates positive and negative scores, as well as polarity and subjectivity scores.\n",
    "Computes various text metrics such as average sentence length, percentage of complex words, Fog Index, and more.\n",
    "\n",
    "## 4.Result Compilation:\n",
    "Stores the results for each article, including sentiment scores and text metrics, in a DataFrame.\n",
    "\n",
    "## 5.DataFrame Operations:\n",
    "Rounds numeric columns in the DataFrame for clarity.\n",
    "Displays the first few rows of the DataFrame.\n",
    "\n",
    "## 6.Output:\n",
    "Saves the DataFrame to a CSV file named 'Output Data.csv.'\n",
    "\n",
    "# Running the .py file:\n",
    "\n",
    "To execute the code and generate the output, follow these steps:\n",
    "\n",
    "- Ensure that Python is installed on your system.\n",
    "- Install the required dependencies by running\n",
    "- Download NLTK data by running the following Python script\n",
    "\n",
    "## Dependencies:\n",
    "\n",
    "- pandas\n",
    "- requests\n",
    "- beautifulsoup4\n",
    "- nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88614b6",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries and Downloading NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a8ba880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca9f9f",
   "metadata": {},
   "source": [
    "# Setting path and Reading Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8c07dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\DELL\\Downloads\" \n",
    "\n",
    "# Reading input data from Excel\n",
    "input_df = pd.read_excel(f\"{path}\\Input.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3258487",
   "metadata": {},
   "source": [
    "# Loading positive,negative and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce0b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading positive and negative words\n",
    "PSF = rf\"{path}\\MasterDictionary\\MasterDictionary\\positive-words.txt\"\n",
    "with open(PSF, 'r') as pos:\n",
    "    positivewords = pos.read().split('\\n')\n",
    "\n",
    "NSF = rf\"{path}\\MasterDictionary\\MasterDictionary\\negative-words.txt\"\n",
    "with open(NSF, 'r', encoding=\"ISO-8859-1\") as neg:\n",
    "    negativewords = neg.read().split('\\n')\n",
    "\n",
    "# Loading stopwords\n",
    "stopwords_files = f\"{path}\\ST\\StopWords\"\n",
    "stopwords_list = []\n",
    "for filename in os.listdir(stopwords_files):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(stopwords_files, filename), 'r', encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                words = line.split('|')[0].strip()\n",
    "                stopwords_list.append(words)\n",
    "\n",
    "stopwords_list = list(set(stopwords_list))\n",
    "stopwords_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4be860",
   "metadata": {},
   "source": [
    "# Data Processing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ac010de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list to store the result data for each article.\n",
    "result_data = []\n",
    "\n",
    "# Looping through each row in the input dataset\n",
    "for index, row in input_df.iterrows():\n",
    "    article_url = row['URL']\n",
    "\n",
    "    # Sending a HTTP GET request to the article URL and parse the HTML content.\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting the article title from the HTML.\n",
    "    article_title = soup.find('h1')\n",
    "    if article_title:\n",
    "        article_title_text = article_title.text\n",
    "    else:\n",
    "        article_title_text = \"Title not found\"\n",
    "\n",
    "    # Extracting the article text content from the HTML.\n",
    "    article_text_elements = soup.findAll(attrs={'class': 'td-post-content'})\n",
    "    if article_text_elements:\n",
    "        article_text = article_text_elements[0].text.replace('\\n', \" \")\n",
    "    else:\n",
    "        article_text = \"Content not found\"\n",
    "\n",
    "    # Removing punctuation from the article text.\n",
    "    article_text1 = article_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenizing the cleaned article text into words.\n",
    "    text_tokens = word_tokenize(article_text1)\n",
    "\n",
    "    # Removing stopwords from the tokenized text.\n",
    "    nostopwords_tokens = [word for word in text_tokens if not word in stopwords_list]\n",
    "\n",
    "    # Calculating the number of words in the text.\n",
    "    words_count = len(nostopwords_tokens)\n",
    "\n",
    "    # Calculating positive and negative scores using dictionary.\n",
    "    Positive_Score = sum(1 for word in nostopwords_tokens if word in positivewords)\n",
    "    Negative_Score = sum(1 for word in nostopwords_tokens if word in negativewords)\n",
    "\n",
    "    # Calculating Polarity and Subjectivity Scores.\n",
    "    Polarity_Score = (Positive_Score - Negative_Score) / ((Positive_Score + Negative_Score) + 0.000001)\n",
    "    Subjectivity_Score = (Positive_Score + Negative_Score) / ((words_count) + 0.000001)\n",
    "    def cal_avg_sentence_length(text):\n",
    "        total_characters = len(re.sub(r'\\s', '', text))\n",
    "        total_sentences = len(re.split(r'[?!.]', text))\n",
    "        avg_sentence_length = total_characters / total_sentences if total_sentences > 0 else 0\n",
    "        return avg_sentence_length\n",
    "    Average_Sentence_Length = cal_avg_sentence_length(article_text)\n",
    "      # Define functions to calculate average number of words per sentence.\n",
    "    def cal_avg_number_of_words_per_sentence(text):\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        avg_number_of_words_per_sentence = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "        return avg_number_of_words_per_sentence\n",
    "    Average_Number_Of_Words_Per_Sentence = cal_avg_number_of_words_per_sentence(article_text)\n",
    "      # Define functions to calculate average number of words per sentence.\n",
    "    def cal_avg_number_of_words_per_sentence(text):\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        avg_number_of_words_per_sentence = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "        return avg_number_of_words_per_sentence\n",
    "    Average_Number_Of_Words_Per_Sentence = cal_avg_number_of_words_per_sentence(article_text)\n",
    "\n",
    "  # Define a function to count syllables in a word.\n",
    "    def count_syllables(word):\n",
    "        vowels = \"AEIOUaeiou\"\n",
    "        syllable_count = 0\n",
    "        prev_char_was_vowel = False\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    syllable_count += 1\n",
    "                    prev_char_was_vowel = True\n",
    "                else:\n",
    "                    prev_char_was_vowel = False\n",
    "        if word.endswith(('e', 'E')) and not word.endswith(('ed', 'es')):    # Adjustment for words ending with \"ed\" and \"es\"\n",
    "            syllable_count -= 1\n",
    "        syllable_count = max(1, syllable_count)\n",
    "        return syllable_count\n",
    "    words = re.findall(r'\\b\\w+\\b', article_text)\n",
    "\n",
    "  # Counting the number of complex words based on syllable count.\n",
    "    Complex_Word_Count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "  # Calculating the average syllables per word based on syllable count.\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    Syllable_Per_Word = sum(syllable_counts)/len(syllable_counts)\n",
    "  # Calculating the word count after removing stopwords using stopwords class of nltk package.\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_count = [word for word in text_tokens if not word in stop_words]\n",
    "    Word_Count = len(word_count)\n",
    "\n",
    "  # Calculating the percentage of complex words.\n",
    "    Percentage_of_Complex_Words = (Complex_Word_Count / Word_Count) * 100\n",
    "\n",
    "  # Calculating the Fog Index.\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_Words)\n",
    "\n",
    "  # Count personal pronouns in the article text.\n",
    "    pronouns = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    pronoun_matches = re.findall(pronouns, article_text, flags=re.IGNORECASE)\n",
    "    filtered_pronouns = [pronoun for pronoun in pronoun_matches if pronoun.lower() != \"us\"]\n",
    "    Personal_Pronouns = len(filtered_pronouns)\n",
    "\n",
    "  # Calculate the average word length.\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(article_text.split())\n",
    "    Average_Word_Length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Extract URL_ID and URL from the current row and append the result data to the list.\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    result_data.append({\n",
    "        'URL_ID': url_id,\n",
    "        'URL': url,\n",
    "        'POSITIVE SCORE': Positive_Score,\n",
    "        'NEGATIVE SCORE': Negative_Score,\n",
    "        'POLARITY SCORE': Polarity_Score,\n",
    "        'SUBJECTIVITY SCORE': Subjectivity_Score,\n",
    "        'AVG SENTENCE LENGTH': Average_Sentence_Length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': Percentage_of_Complex_Words,\n",
    "        'FOG INDEX': Fog_Index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': Average_Number_Of_Words_Per_Sentence,\n",
    "        'COMPLEX WORD COUNT': Complex_Word_Count,\n",
    "        'WORD COUNT': Word_Count,\n",
    "        'SYLLABLE PER WORD': Syllable_Per_Word,\n",
    "        'PERSONAL PRONOUNS': Personal_Pronouns,\n",
    "        'AVG WORD LENGTH': Average_Word_Length\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb78f5b",
   "metadata": {},
   "source": [
    "# Rounds numeric columns and Displays the first few rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dc7611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the result data.\n",
    "results_df = pd.DataFrame(result_data)\n",
    "\n",
    "# Round numeric columns\n",
    "for column in results_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(results_df[column]):\n",
    "        results_df[column] = results_df[column].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e5b58",
   "metadata": {},
   "source": [
    "# Display output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c74e9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.061</td>\n",
       "      <td>72.772</td>\n",
       "      <td>2.561</td>\n",
       "      <td>30.133</td>\n",
       "      <td>15.468</td>\n",
       "      <td>19</td>\n",
       "      <td>742</td>\n",
       "      <td>1.190</td>\n",
       "      <td>11</td>\n",
       "      <td>4.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.103</td>\n",
       "      <td>100.402</td>\n",
       "      <td>7.104</td>\n",
       "      <td>43.003</td>\n",
       "      <td>17.841</td>\n",
       "      <td>66</td>\n",
       "      <td>929</td>\n",
       "      <td>1.332</td>\n",
       "      <td>3</td>\n",
       "      <td>5.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.089</td>\n",
       "      <td>117.351</td>\n",
       "      <td>12.430</td>\n",
       "      <td>51.912</td>\n",
       "      <td>18.702</td>\n",
       "      <td>89</td>\n",
       "      <td>716</td>\n",
       "      <td>1.414</td>\n",
       "      <td>12</td>\n",
       "      <td>6.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>35</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.164</td>\n",
       "      <td>124.288</td>\n",
       "      <td>8.616</td>\n",
       "      <td>53.162</td>\n",
       "      <td>20.269</td>\n",
       "      <td>61</td>\n",
       "      <td>708</td>\n",
       "      <td>1.362</td>\n",
       "      <td>5</td>\n",
       "      <td>5.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.078</td>\n",
       "      <td>96.775</td>\n",
       "      <td>7.745</td>\n",
       "      <td>41.808</td>\n",
       "      <td>17.050</td>\n",
       "      <td>34</td>\n",
       "      <td>439</td>\n",
       "      <td>1.290</td>\n",
       "      <td>6</td>\n",
       "      <td>5.543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              33               5           0.737               0.061   \n",
       "1              56              31           0.287               0.103   \n",
       "2              37              23           0.233               0.089   \n",
       "3              35              71          -0.340               0.164   \n",
       "4              21               8           0.448               0.078   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0               72.772                        2.561     30.133   \n",
       "1              100.402                        7.104     43.003   \n",
       "2              117.351                       12.430     51.912   \n",
       "3              124.288                        8.616     53.162   \n",
       "4               96.775                        7.745     41.808   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                            15.468                  19         742   \n",
       "1                            17.841                  66         929   \n",
       "2                            18.702                  89         716   \n",
       "3                            20.269                  61         708   \n",
       "4                            17.050                  34         439   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0              1.190                 11            4.582  \n",
       "1              1.332                  3            5.478  \n",
       "2              1.414                 12            6.117  \n",
       "3              1.362                  5            5.972  \n",
       "4              1.290                  6            5.543  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7d830",
   "metadata": {},
   "source": [
    "# Save as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31e1aa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('Output Data.csv', index=False)\n",
    "print(\"CSV File Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07742ee8",
   "metadata": {},
   "source": [
    "# Final Executable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86abb0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "path = r\"C:\\Users\\DELL\\Downloads\"  \n",
    "\n",
    "# Read input data from Excel\n",
    "input_df = pd.read_excel(f\"{path}\\Input.xlsx\")\n",
    "\n",
    "# Load positive and negative words\n",
    "PSF = rf\"{path}\\MasterDictionary\\MasterDictionary\\positive-words.txt\"\n",
    "with open(PSF, 'r') as pos:\n",
    "    positivewords = pos.read().split('\\n')\n",
    "\n",
    "NSF = rf\"{path}\\MasterDictionary\\MasterDictionary\\negative-words.txt\"\n",
    "with open(NSF, 'r', encoding=\"ISO-8859-1\") as neg:\n",
    "    negativewords = neg.read().split('\\n')\n",
    "\n",
    "# Load stopwords\n",
    "stopwords_files = f\"{path}\\ST\\StopWords\"\n",
    "stopwords_list = []\n",
    "for filename in os.listdir(stopwords_files):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(stopwords_files, filename), 'r', encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                words = line.split('|')[0].strip()\n",
    "                stopwords_list.append(words)\n",
    "\n",
    "stopwords_list = list(set(stopwords_list))\n",
    "stopwords_list.sort()\n",
    "\n",
    "# Create an empty list to store the result data for each article.\n",
    "result_data = []\n",
    "\n",
    "# Loop through each row in the input dataset\n",
    "for index, row in input_df.iterrows():\n",
    "    article_url = row['URL']\n",
    "\n",
    "    # Send an HTTP GET request to the article URL and parse the HTML content.\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the article title from the HTML.\n",
    "    article_title = soup.find('h1')\n",
    "    if article_title:\n",
    "        article_title_text = article_title.text\n",
    "    else:\n",
    "        article_title_text = \"Title not found\"\n",
    "\n",
    "    # Extract the article text content from the HTML.\n",
    "    article_text_elements = soup.findAll(attrs={'class': 'td-post-content'})\n",
    "    if article_text_elements:\n",
    "        article_text = article_text_elements[0].text.replace('\\n', \" \")\n",
    "    else:\n",
    "        article_text = \"Content not found\"\n",
    "\n",
    "    # Remove punctuation from the article text.\n",
    "    article_text1 = article_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the cleaned article text into words.\n",
    "    text_tokens = word_tokenize(article_text1)\n",
    "\n",
    "    # Remove stopwords from the tokenized text.\n",
    "    nostopwords_tokens = [word for word in text_tokens if not word in stopwords_list]\n",
    "\n",
    "    # Calculate the number of words in the text.\n",
    "    words_count = len(nostopwords_tokens)\n",
    "\n",
    "    # Calculate positive and negative scores using dictionary.\n",
    "    Positive_Score = sum(1 for word in nostopwords_tokens if word in positivewords)\n",
    "    Negative_Score = sum(1 for word in nostopwords_tokens if word in negativewords)\n",
    "\n",
    "    # Calculate Polarity and Subjectivity Scores.\n",
    "    Polarity_Score = (Positive_Score - Negative_Score) / ((Positive_Score + Negative_Score) + 0.000001)\n",
    "    Subjectivity_Score = (Positive_Score + Negative_Score) / ((words_count) + 0.000001)\n",
    "    def cal_avg_sentence_length(text):\n",
    "        total_characters = len(re.sub(r'\\s', '', text))\n",
    "        total_sentences = len(re.split(r'[?!.]', text))\n",
    "        avg_sentence_length = total_characters / total_sentences if total_sentences > 0 else 0\n",
    "        return avg_sentence_length\n",
    "    Average_Sentence_Length = cal_avg_sentence_length(article_text)\n",
    "      # Define functions to calculate average number of words per sentence.\n",
    "    def cal_avg_number_of_words_per_sentence(text):\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        avg_number_of_words_per_sentence = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "        return avg_number_of_words_per_sentence\n",
    "    Average_Number_Of_Words_Per_Sentence = cal_avg_number_of_words_per_sentence(article_text)\n",
    "      # Define functions to calculate average number of words per sentence.\n",
    "    def cal_avg_number_of_words_per_sentence(text):\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        avg_number_of_words_per_sentence = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "        return avg_number_of_words_per_sentence\n",
    "    Average_Number_Of_Words_Per_Sentence = cal_avg_number_of_words_per_sentence(article_text)\n",
    "\n",
    "  # Define a function to count syllables in a word.\n",
    "    def count_syllables(word):\n",
    "        vowels = \"AEIOUaeiou\"\n",
    "        syllable_count = 0\n",
    "        prev_char_was_vowel = False\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    syllable_count += 1\n",
    "                    prev_char_was_vowel = True\n",
    "                else:\n",
    "                    prev_char_was_vowel = False\n",
    "        if word.endswith(('e', 'E')) and not word.endswith(('ed', 'es')):    # Adjustment for words ending with \"ed\" and \"es\"\n",
    "            syllable_count -= 1\n",
    "        syllable_count = max(1, syllable_count)\n",
    "        return syllable_count\n",
    "    words = re.findall(r'\\b\\w+\\b', article_text)\n",
    "\n",
    "  # Count the number of complex words based on syllable count.\n",
    "    Complex_Word_Count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "  # Calculate the average syllables per word based on syllable count.\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    Syllable_Per_Word = sum(syllable_counts)/len(syllable_counts)\n",
    "  # Calculate the word count after removing stopwords using stopwords class of nltk package.\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_count = [word for word in text_tokens if not word in stop_words]\n",
    "    Word_Count = len(word_count)\n",
    "\n",
    "  # Calculate the percentage of complex words.\n",
    "    Percentage_of_Complex_Words = (Complex_Word_Count / Word_Count) * 100\n",
    "\n",
    "  # Calculate the Fog Index.\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_Words)\n",
    "\n",
    "  # Count personal pronouns in the article text.\n",
    "    pronouns = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    pronoun_matches = re.findall(pronouns, article_text, flags=re.IGNORECASE)\n",
    "    filtered_pronouns = [pronoun for pronoun in pronoun_matches if pronoun.lower() != \"us\"]\n",
    "    Personal_Pronouns = len(filtered_pronouns)\n",
    "\n",
    "  # Calculate the average word length.\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(article_text.split())\n",
    "    Average_Word_Length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Extract URL_ID and URL from the current row and append the result data to the list.\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    result_data.append({\n",
    "        'URL_ID': url_id,\n",
    "        'URL': url,\n",
    "        'POSITIVE SCORE': Positive_Score,\n",
    "        'NEGATIVE SCORE': Negative_Score,\n",
    "        'POLARITY SCORE': Polarity_Score,\n",
    "        'SUBJECTIVITY SCORE': Subjectivity_Score,\n",
    "        'AVG SENTENCE LENGTH': Average_Sentence_Length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': Percentage_of_Complex_Words,\n",
    "        'FOG INDEX': Fog_Index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': Average_Number_Of_Words_Per_Sentence,\n",
    "        'COMPLEX WORD COUNT': Complex_Word_Count,\n",
    "        'WORD COUNT': Word_Count,\n",
    "        'SYLLABLE PER WORD': Syllable_Per_Word,\n",
    "        'PERSONAL PRONOUNS': Personal_Pronouns,\n",
    "        'AVG WORD LENGTH': Average_Word_Length\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the result data.\n",
    "results_df = pd.DataFrame(result_data)\n",
    "\n",
    "# Round numeric columns\n",
    "for column in results_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(results_df[column]):\n",
    "        results_df[column] = results_df[column].round(3)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "results_df.head()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('Output Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278f10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
